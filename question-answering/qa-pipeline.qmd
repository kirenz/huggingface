---
title: "Question Answering"
lang: en
subtitle: "Hugging Face ðŸ¤— Pipeline"
author: Jan Kirenz
execute:
  eval: false
  echo: true
highlight-style: github
format:
  revealjs: 
    toc: true
    toc-depth: 1
    embed-resources: false
    theme: [dark, ../custom.scss]  
    incremental: true
    transition: slide
    transition-speed: slow
    background-transition: fade
    code-copy: true
    code-line-numbers: true
    smaller: false
    scrollable: true
    slide-number: c
    preview-links: auto
    chalkboard: 
      buttons: false
    #logo: images/logo.png
    footer: "Source: [Hugging Face](https://huggingface.co/) | Jan Kirenz"
---


# Prerequisites

## Installation

- You need Anaconda or Miniconda

- Install this virtual environment [env-transformers.yml](https://github.com/kirenz/environments) 


## Python setup

For the simple pipeline model

```{python}
from transformers import pipeline
```

Fine-Tuning example:

```{python}

from datasets import load_dataset

from transformers import TFAutoModelForQuestionAnswering
from transformers import create_optimizer
from transformers import DefaultDataCollator
from transformers import AutoTokenizer

import tensorflow as tf
```



# Basics

The follwoing content is based on the following content:

- [Hugging Face question answering task guide](https://huggingface.co/docs/transformers/tasks/question_answering)

- [Hugging Face question answering overview](https://huggingface.co/tasks/question-answering)

## Intuition

![](images/qa.png)

- Question answering tasks return an answer given a question. 


## Use Cases


- Automate the response to Frequently Asked Questions

- Chat Bots


## Exctractive vs generative

- *Extractive*: selecting an answer directly from the original text


- *Generative*: involves generating a new answer that may not be directly present in the text

## Common types

- *Extractive* QA: The model extracts the answer from the original context.^[The context could be a provided text, a table or even HTML! This is usually solved with BERT-like models.]

- *Open Generative* QA: The model generates free text directly based on the context. 

- *Closed Generative QA*: In this case, no context is provided. The answer is completely generated by a model.

## Closed-domain vs open-domain

- Closed-domain models are restricted to a specific domain (e.g. legal, medical documents).

- Open-domain models are not restricted to a specific domain.


# Simple Pipeline Model

- Define the Pipeline^[Will be initialized with the default model `distilbert-base-cased-distilled-squad`]

. . .

```{python}
qa_model = pipeline("question-answering")

```

- Provide question and context

. . .

```{python}

question = "Where do I live?"

context = "My name is Jan and I live in Stuttgart."

```

- Return the answer
. . .

```{python}

qa_model(question=question, context=context)

```

. . .

```{bash}
{'score': 0.9671180844306946, 'start': 29, 'end': 38, 'answer': 'Stuttgart'}

```

